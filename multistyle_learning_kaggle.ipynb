{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install gdown\nimport gdown\n!gdown --id 1Dw3yOdKEEsKdc42isVt118AofD3wE-qH -O images.zip\n!unzip images.zip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import namedtuple\n\nimport torch\nimport torchvision.models.vgg as vgg\n\nLossOutput = namedtuple(\n    \"LossOutput\", [\"relu1\", \"relu2\", \"relu3\", \"relu4\", \"relu5\"])\n\n\nclass LossNetwork(torch.nn.Module):\n    \"\"\"Reference:\n        https://discuss.pytorch.org/t/how-to-extract-features-of-an-image-from-a-trained-model/119/3\n    \"\"\"\n\n    def __init__(self):\n        super(LossNetwork, self).__init__()\n        self.vgg_layers = vgg.vgg19(pretrained=True).features\n        self.layer_name_mapping = {\n            '3': \"relu1\",\n            '8': \"relu2\",\n            '17': \"relu3\",\n            '26': \"relu4\",\n            '35': \"relu5\",\n        }\n\n    def forward(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n        return LossOutput(**output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\n\nclass TransformerNet(torch.nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)     \n        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n        # Residual layers\n        self.res1 = ResidualBlock(128)\n        self.res2 = ResidualBlock(128)\n        self.res3 = ResidualBlock(128)\n        self.res4 = ResidualBlock(128)\n        self.res5 = ResidualBlock(128)\n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(\n            128, 64, kernel_size=3, stride=1, upsample=2)\n        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n        self.deconv2 = UpsampleConvLayer(\n            64, 32, kernel_size=3, stride=1, upsample=2)\n        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n        # Non-linearities\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, X):\n        y = self.relu(self.in1(self.conv1(X)))\n        y = self.relu(self.in2(self.conv2(y)))\n        y = self.relu(self.in3(self.conv3(y)))\n        y = self.res1(y)\n        y = self.res2(y)\n        y = self.res3(y)\n        y = self.res4(y)\n        y = self.res5(y)\n        y = self.relu(self.in4(self.deconv1(y)))\n        y = self.relu(self.in5(self.deconv2(y)))\n        y = self.deconv3(y)\n        return y\n\n\nclass ConvLayer(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(torch.nn.Module):\n    \"\"\"ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    \"\"\"\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n\n\nclass UpsampleConvLayer(torch.nn.Module):\n    \"\"\"UpsampleConvLayer\n    Upsamples the input and then does a convolution. This method gives better results\n    compared to ConvTranspose2d.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        if upsample:\n            self.upsample_layer = torch.nn.Upsample(\n                mode='nearest', scale_factor=upsample)\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(\n            in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = self.upsample_layer(x_in)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn import ModuleList, InstanceNorm2d\n\nclass TransformerMultiStyleNet(torch.nn.Module):\n    def __init__(self, style_number = 1):\n        super().__init__()\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n        self.in1 = ModuleList([InstanceNorm2d(32, affine=True) for _ in range(style_number)])        \n        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n        self.in2 = ModuleList([InstanceNorm2d(64, affine=True) for _ in range(style_number)])  \n        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n        self.in3 = ModuleList([InstanceNorm2d(128, affine=True) for _ in range(style_number)])  \n        # Residual layers\n        self.res1 = ResidualMultiStyleBlock(128, style_number)\n        self.res2 = ResidualMultiStyleBlock(128, style_number)\n        self.res3 = ResidualMultiStyleBlock(128, style_number)\n        self.res4 = ResidualMultiStyleBlock(128, style_number)\n        self.res5 = ResidualMultiStyleBlock(128, style_number)\n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(\n            128, 64, kernel_size=3, stride=1, upsample=2)\n        self.in4 = ModuleList([InstanceNorm2d(64, affine=True) for _ in range(style_number)])  \n        self.deconv2 = UpsampleConvLayer(\n            64, 32, kernel_size=3, stride=1, upsample=2)\n        self.in5 = ModuleList([InstanceNorm2d(32, affine=True) for _ in range(style_number)])  \n        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n        # Non-linearities\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, X, style):\n        y = self.relu(self.in1[style](self.conv1(X)))\n        y = self.relu(self.in2[style](self.conv2(y)))\n        y = self.relu(self.in3[style](self.conv3(y)))\n        y = self.res1(y, style)\n        y = self.res2(y, style)\n        y = self.res3(y, style)\n        y = self.res4(y, style)\n        y = self.res5(y, style)\n        y = self.relu(self.in4[style](self.deconv1(y)))\n        y = self.relu(self.in5[style](self.deconv2(y)))\n        y = self.deconv3(y)\n        return y\n\n\nclass ResidualMultiStyleBlock(torch.nn.Module):\n    \"\"\"ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    \"\"\"\n\n    def __init__(self, channels, style_number=1):\n        super().__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = ModuleList([InstanceNorm2d(channels, affine=True) for _ in range(style_number)])  \n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = ModuleList([InstanceNorm2d(channels, affine=True) for _ in range(style_number)])  \n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x, style):\n        residual = x\n        out = self.relu(self.in1[style](self.conv1(x)))\n        out = self.in2[style](self.conv2(out))\n        out = out + residual\n        return out\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\nimport torch\nfrom torchvision import models, transforms\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nfrom torch.nn import Sequential, ReLU, Conv2d, AvgPool2d, ReflectionPad2d,\\\nBatchNorm2d, InstanceNorm2d, UpsamplingNearest2d\nfrom torch.nn import ConvTranspose2d as ConvT2d\n\nclass Pyramid(nn.Module):\n  def __init__(self, mode=None):\n    super().__init__()\n    ratios = [32, 16, 8, 4, 2, 1]\n    mode = 5\n    if mode == 5:\n        ratios = ratios[1:]\n    self.stages = len(ratios)\n    self.mode = mode\n    do_inplace = False  # True\n    self.downs = nn.ModuleList([ReLU()]*self.stages) #\n    self.ups   = nn.ModuleList([ReLU()]*self.stages) #\n    norm = InstanceNorm2d\n    features = 8*1\n    act = ReLU#nn.LeakyReLU #ReLU\n    class down_com(nn.Module):\n      def __init__(self, layer_number):\n        super().__init__()\n        self.layer = Sequential(\n          AvgPool2d(ratios[layer_number], ratios[layer_number]),\n          Conv2d(3, features, 3, padding=1, padding_mode='reflect'), norm(features), act(do_inplace),\n          Conv2d(features, features, 3, padding=1, padding_mode='reflect'), norm(features), act(do_inplace),\n          Conv2d(features, features, 1), norm(features), act(do_inplace),\n          #norm(features)\n        )\n      \n    #self.downsdown_com(0).layer)\n     #self.downs[0] = down_com(0).layer  # Sequential(#norm(3),\n        #*down_com(0).layer[:-1], UpsamplingNearest2d(scale_factor=2), norm(features))\n    for layer_number in range(0, self.stages):\n      self.downs[layer_number] = down_com(layer_number).layer\n      #self.downs.append(down_com(layer_number).layer)\n    \n    class up_com(nn.Module):\n      def __init__(self, features_num):\n        super().__init__()\n        self.layer = Sequential(\n                norm(features_num),\n                Conv2d(features_num, features_num, 3, padding=1, padding_mode='reflect'), norm(features_num), act(do_inplace),\n                Conv2d(features_num, features_num, 3, padding=1, padding_mode='reflect'), norm(features_num), act(do_inplace),\n                Conv2d(features_num, features_num, 1), norm(features_num), act(do_inplace),\n                #UpsamplingNearest2d(scale_factor=2),\n                \n                #Conv2d(features_num, features_num, 3, padding=1, padding_mode='reflect'), norm(features_num), act(do_inplace),\n                UpsamplingNearest2d(scale_factor=2),\n                #Conv2d(features_num, features_num, 3, padding=1, padding_mode='reflect'), norm(features_num), act(do_inplace),\n                )\n        \n    for layer_number in range(self.stages):\n      features_num = (layer_number + 1)*features\n      self.ups[layer_number] = up_com(features_num).layer\n      #self.ups.append(up_com(features_num).layer)\n    self.ups[layer_number] = Sequential(*self.ups[layer_number][:-1], Conv2d(features_num, 3, 1))\n    \n  def forward(self, image):\n    last_out = None\n    #image = self.initial_bn(image.clone())\n    for stage in range(self.stages):\n      #print(f'stage={stage}, {image.device}')\n      \n      cur_x = self.downs[stage](image.clone())\n      #print('inp = ', image.shape, f'down({stage})=', cur_x.shape)\n      tocat = [cur_x]\n      if last_out is not None:\n        tocat.append(last_out)\n      #print(f'{[x.shape for x in tocat]}')\n      cur_x = torch.cat(tocat, dim=1)\n      last_out = self.ups[stage](cur_x) # del clone\n      #print(cur_x.shape, last_out.shape)\n    return last_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom torchvision import transforms\n\n\ndef gram_matrix(y):\n    (b, ch, h, w) = y.size()\n    features = y.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (h * w) # remove /ch\n    return gram\n\n\ndef tensor_normalizer():\n    return transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225])\n\n\ndef recover_image(img):\n    return (\n        (\n            img *\n            np.array([0.229, 0.224, 0.225]).reshape((1, 3, 1, 1)) +\n            np.array([0.485, 0.456, 0.406]).reshape((1, 3, 1, 1))\n        ).transpose(0, 2, 3, 1) *\n        255.\n    ).clip(0, 255).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport time \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n# For getting VGG model\nimport torchvision.models.vgg as vgg\nimport torch.utils.model_zoo as model_zoo\n# Image transformation pipeline\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nfrom PIL import Image, ImageFile\nfrom tqdm import tqdm_notebook\n\n'''from fast_neural_style.transformer_net import TransformerNet\nfrom fast_neural_style.utils import (\n    gram_matrix, recover_image, tensor_normalizer\n)\nfrom fast_neural_style.loss_network import LossNetwork\n''' \nget_ipython().run_line_magic('matplotlib', 'inline')\nImageFile.LOAD_TRUNCATED_IMAGES = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if False:\n    SEED = 1080\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(SEED)\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    else:\n        torch.set_default_tensor_type('torch.FloatTensor')\n\nif torch.cuda.is_available():\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    kwargs = {'num_workers': 4, 'pin_memory': True}\nelse:\n    torch.set_default_tensor_type('torch.FloatTensor')\n    kwargs = {}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 256//1\nBATCH_SIZE = 4\nDATASET = \"../input/coco2017/\"\ntransform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE), \n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(), tensor_normalizer()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder\ntrain_dataset = datasets.ImageFolder(DATASET, transform)\n# http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#torch.save(LossNetwork.state_dict(), './LossNet.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nwith torch.no_grad():\n    loss_network = LossNetwork()\n    loss_network.to(device)\nloss_network.eval()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"styles = list(Path('./style_images').glob('*.*'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif False:\n    STYLE_IMAGE = \"./style_images/rain-princess.jpg\"\n    style_img = Image.open(STYLE_IMAGE).convert('RGB')\n    with torch.no_grad():\n        style_img_tensor = transforms.Compose([\n            transforms.Resize(IMAGE_SIZE* 2),\n            transforms.ToTensor(),\n            tensor_normalizer()]\n        )(style_img).unsqueeze(0)\n        # assert np.sum(style_img - recover_image(style_img_tensor.numpy())[0].astype(np.uint8)) < 3 * style_img_tensor.size()[2] * style_img_tensor.size()[3]\n        style_img_tensor = style_img_tensor.to(device)\n\n    # http://pytorch.org/docs/master/notes/autograd.html#volatile\n    with torch.no_grad():\n        style_loss_features = loss_network(style_img_tensor)\n        gram_style = [gram_matrix(y) for y in style_loss_features]\n    plt.imshow(recover_image(style_img_tensor.cpu().numpy())[0])\nelse:\n    gram_styles = []\n    for sn, STYLE_IMAGE in enumerate(styles):\n        if sn >= 6:\n            break\n        style_img = Image.open(STYLE_IMAGE).convert('RGB')\n        with torch.no_grad():\n            style_img_tensor = transforms.Compose([\n                transforms.Resize(IMAGE_SIZE* 2),\n                transforms.ToTensor(),\n                tensor_normalizer()]\n            )(style_img).unsqueeze(0)\n            # assert np.sum(style_img - recover_image(style_img_tensor.numpy())[0].astype(np.uint8)) < 3 * style_img_tensor.size()[2] * style_img_tensor.size()[3]\n            style_img_tensor = style_img_tensor.to(device)\n\n        # http://pytorch.org/docs/master/notes/autograd.html#volatile\n        with torch.no_grad():\n            style_loss_features = loss_network(style_img_tensor)\n            gram_styles.append([gram_matrix(y) for y in style_loss_features])\n        plt.imshow(recover_image(style_img_tensor.cpu().numpy())[0])\n        plt.show()\n        style_num = len(gram_styles)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style_loss_features._fields\nfor i in range(len(style_loss_features)):\n    print(i, gram_style[i].numel(), gram_style[i].size())\nfor i in range(len(style_loss_features)):\n    tmp = style_loss_features[i].cpu().numpy()\n    print(i, np.mean(tmp), np.std(tmp))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_debug_image(tensor_orig, tensor_transformed, filename):\n    #assert tensor_orig.size() == tensor_transformed.size(), f'{tensor_orig.size()} != {tensor_transformed.size()}'\n    result = Image.fromarray(recover_image(tensor_transformed.cpu().numpy())[0])\n    orig = Image.fromarray(recover_image(tensor_orig.cpu().numpy())[0])\n    new_im = Image.new('RGB', (result.size[0] * 2 + 5, result.size[1]))\n    new_im.paste(orig, (0,0))\n    new_im.paste(result, (result.size[0] + 5,0))\n    new_im.save(filename)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_ipython().run_line_magic('mkdir', '-p ./debug')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rgrad_params_num(model, model_name=None):\n    rgpn = 0\n    for p in model.parameters():\n        if isinstance(p, BatchNorm2d):\n          print(f'bn: {p}')\n          break\n        if p.requires_grad:\n            rgpn += p.numel()\n    if model_name:\n        print(f'{model_name} req to teach {rgpn} params')\n    return rgpn\n\ndef turn_req_grad_for_instance(model, inst, req_grad=True, modified_layers=0, verbosity=1):\n  for layer in model.children():\n    if isinstance(layer, Sequential) or isinstance(layer, nn.ModuleList):\n      modified_layers += turn_req_grad_for_instance(layer, inst, req_grad, verbosity=0)\n    elif isinstance(layer, inst):\n      layer.requires_grad_(req_grad)\n      modified_layers += 1\n  if verbosity:\n    print(f'modified {modified_layers}')\n  return modified_layers\n\ndef turn_req_grad(params, req_grad=True):\n    for p in params:\n        p.requires_grad = req_grad\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = TransformerMultiStyleNet(style_num) # TransformerNet() # Pyramid() # \nmse_loss = torch.nn.MSELoss()\n# l1_loss = torch.nn.L1Loss()\ntransformer.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nfnames = glob.glob(DATASET + 'val2017/val2017/'+r\"/*.*\")\n#print(f'filenames = {len(fnames)}')\ntransformer = transformer.eval()\nimg_test = Image.open(fnames[np.random.randint(len(fnames))]).convert('RGB')\nplt.imshow(img_test)\n\ntransform = transforms.Compose([\n                                \n                                transforms.ToTensor(),\n                                tensor_normalizer()])\n\nimg_test_tensor = transform(img_test).unsqueeze(0)\nif torch.cuda.is_available():\n    img_test_tensor = img_test_tensor.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.set_default_tensor_type('torch.FloatTensor')\n    \ndef train(steps, base_steps=0):\n    transformer.train()\n    count = 0\n    agg_content_loss = 0.\n    agg_style_loss = 0.\n    agg_reg_loss = 0.   \n    while True:\n        for x, _ in train_loader:\n            x = x.to(device)             \n                        \n            with torch.no_grad():\n                xc = x.detach()\n\n            features_xc = loss_network(xc) # in no grad??\n            with torch.no_grad():\n                f_xc_c = features_xc[2].detach()\n                \n            style_choice = np.random.randint(style_num)\n            gram_style = gram_styles[style_choice]\n                \n            for sub_cnt in range(5):\n                count += 1\n                optimizer.zero_grad()\n            \n                y = transformer(x.clone(), style_choice)            \n                features_y = loss_network(y)\n            \n                content_loss = CONTENT_WEIGHT * mse_loss(features_y[2], f_xc_c)\n                \n                reg_loss = 0\n                #reg_loss = REGULARIZATION * (\n                #    torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \n                #    torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :])))\n\n                style_loss = 0.\n                for l, weight in enumerate(STYLE_WEIGHTS):\n                    gram_s = gram_style[l]\n                    gram_y = gram_matrix(features_y[l])\n                    style_loss += float(weight) * mse_loss(gram_y, gram_s.expand_as(gram_y))\n                \n                total_loss = content_loss + style_loss# + reg_loss \n                total_loss.backward()\n                optimizer.step()\n\n                agg_content_loss += content_loss\n                agg_style_loss += style_loss\n                agg_reg_loss += reg_loss\n\n            if count % LOG_INTERVAL == 0:\n                mesg = \"like {} {} [{}/{}] content: {:.2f}  style: {:.2f}  reg: {:.2f} total: {:.6f}\".format(\n                            styles[style_choice].name, time.ctime(), count, steps,\n                            agg_content_loss / LOG_INTERVAL,\n                            agg_style_loss / LOG_INTERVAL,\n                            agg_reg_loss / LOG_INTERVAL,\n                            (agg_content_loss + agg_style_loss + \n                             agg_reg_loss ) / LOG_INTERVAL\n                        )\n                print(mesg)\n                agg_content_loss = 0.\n                agg_style_loss = 0.\n                agg_reg_loss = 0.\n                agg_stable_loss = 0.\n                transformer.eval()\n                y = transformer(img_test_tensor.clone(), style_choice)\n                #save_debug_image(img_test_tensor, y.detach(), f\"./debug/{base_steps + count:0>4d}.png\")\n                plt.imshow(recover_image(y.detach().cpu().numpy())[0])\n                plt.show()\n\n                #transformer.train()\n                \n            if count >= steps:\n                return\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil, os\nfrom pathlib import Path\nif Path('./debug').exists():\n    shutil.rmtree('./debug')\nos.mkdir('./debug')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CONTENT_WEIGHT = 1\n#STYLE_WEIGHTS = np.array([1e-1, 1, 1e1, 5, 1e1]) * 1e4\nSTYLE_WEIGHTS = np.array([1e3]*5)\nREGULARIZATION = 1e-6\nLOG_INTERVAL = 200\n\nLR = 1e-5\noptimizer = Adam(transformer.parameters(), LR)\nprint(len(train_loader))\ntrain(200, 0)\nsave_model_path = \"rain_unstable_vgg19.pth\"\ntorch.save(transformer.state_dict(), save_model_path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Path('./debug.zip').unlink()\n!zip -r debug.zip ./debug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print([f'{x:0>2d}' for x in range(0,120,14)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image.fromarray(recover_image(img_output.data.cpu().numpy())[0])\n\n\n\ntransform = transforms.Compose([\n                                transforms.Resize(IMAGE_SIZE),\n                                transforms.ToTensor(),\n                                tensor_normalizer()])\nimg_tensor = transform(img).unsqueeze(0)\nif torch.cuda.is_available():\n    img_tensor = img_tensor.cuda()\n\nimg_output = transformer(Variable(img_tensor, volatile=True))\nImage.fromarray(recover_image(img_output.data.cpu().numpy())[0])\n\n\n# In[36]:\n\n\nimg = Image.open(\"./content_images/amber.jpg\").convert('RGB')\ntransform = transforms.Compose([\n    transforms.Resize(512),\n    transforms.ToTensor(),\n    tensor_normalizer()])\nimg_tensor = transform(img).unsqueeze(0)\nprint(img_tensor.size())\nif torch.cuda.is_available():\n    img_tensor = img_tensor.cuda()\n\nimg_output = transformer(Variable(img_tensor, volatile=True))\nplt.imshow(recover_image(img_tensor.cpu().numpy())[0])\n\n\n# In[37]:\n\n\nplt.imshow(recover_image(img_output.data.cpu().numpy())[0])\n\n\n# In[38]:\n\n\nimg = Image.open(\"./content_images/amber.jpg\").convert('RGB')\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.ToTensor(),\n    tensor_normalizer()])\nimg_tensor = transform(img).unsqueeze(0)\nprint(img_tensor.size())\nif torch.cuda.is_available():\n    img_tensor = img_tensor.cuda()\n\nimg_output = transformer(Variable(img_tensor, volatile=True))\nplt.imshow(recover_image(img_output.data.cpu().numpy())[0])\n\n\n# In[32]:\n\n\noutput_img = Image.fromarray(recover_image(img_output.data.cpu().numpy())[0])\noutput_img.save(\"amber.png\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}